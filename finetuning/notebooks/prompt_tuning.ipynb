{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df4910d-bc02-468d-a921-b0b4e88bcd80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T01:52:06.885875900Z",
     "start_time": "2023-09-30T01:52:06.833108800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\admin\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f4e582-b911-4d70-aece-cd7bafba7436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:46.308449900Z",
     "start_time": "2023-09-30T18:07:34.322881900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin D:\\CondaEnvs\\LLM_Tuning\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from collections.abc import Iterator\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "from utils import (\n",
    "    llama2_chat_text_convert_train, \n",
    "    llama2_chat_text_convert_test, \n",
    "    print_trainable_parameters, \n",
    "    bnb_config,\n",
    "    transformer_trainer, \n",
    "    sft_trainer,\n",
    "    max_length,\n",
    "    soft_prompt_config,\n",
    "    hard_prompt_config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26d16f-0b03-4155-9b40-5d830af0377f",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_file = '../../data/data.csv'\n",
    "test_file = '../../data/data.csv'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_file, 'test': test_file})\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:47.842776600Z",
     "start_time": "2023-09-30T18:07:47.337152800Z"
    }
   },
   "id": "42e3c6f45124af83"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_text', 'output_text'],\n    num_rows: 299\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:47.853375400Z",
     "start_time": "2023-09-30T18:07:47.846762500Z"
    }
   },
   "id": "239ca44d1634e219"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca9873c-e04c-4d9e-8fd4-56df0e31f5ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:48.053055900Z",
     "start_time": "2023-09-30T18:07:48.016496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_text', 'output_text'],\n    num_rows: 299\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modify data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e09146874a95371a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9608980c-aea3-469b-a057-c524fb583cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:48.799478Z",
     "start_time": "2023-09-30T18:07:48.743130800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x: llama2_chat_text_convert_train(x, input_col=\"input_text\", output_col=\"output_text\"), \n",
    "                                  remove_columns=['input_text', 'output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81705ace-87c5-4924-8cb9-4c2e0ecc0ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:48.988871300Z",
     "start_time": "2023-09-30T18:07:48.961879900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'instruction': '<s>[INST] How much did my fleet idle last month? [/INST]',\n 'text': '<s>[INST] How much did my fleet idle last month? [/INST] Idling-Idling duration </s></s>',\n 'label': 'Idling-Idling duration </s></s>'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a183f8d-8da2-4417-920f-0c9b528bf3cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:49.361857900Z",
     "start_time": "2023-09-30T18:07:49.233638400Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(lambda x: llama2_chat_text_convert_test(x, input_col=\"input_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66b28e6-3758-4dfa-bbb0-34b9620cd2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:49.531490200Z",
     "start_time": "2023-09-30T18:07:49.483753500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_text': 'How much did my fleet idle last month?',\n 'output_text': 'Idling-Idling duration',\n 'text': '<s>[INST] How much did my fleet idle last month? [/INST]'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f4f9c-3f6e-4a7c-b4b2-b3fdbcc958ea",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2e4a28-95a8-428e-a148-010df89bfda3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:50.126069800Z",
     "start_time": "2023-09-30T18:07:50.104994900Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de021ced-750f-4d3b-ad2e-5ed1711608a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:07:50.365785400Z",
     "start_time": "2023-09-30T18:07:50.333271Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-xin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf23e79-213d-4885-be9c-b4087bc61cb1",
   "metadata": {},
   "source": [
    "## load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a010afbc-eccb-4577-91bb-b84704973b61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T18:08:09.416414900Z",
     "start_time": "2023-09-30T18:07:50.757832900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "882e2ec4a84846889a47d42263263981"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map='auto', \n",
    "                                             trust_remote_code=True)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 409,600 || all params: 6,738,825,216 || trainable%: 0.006078210769252277\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model, soft_prompt_config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T18:08:09.465819300Z",
     "start_time": "2023-09-30T18:08:09.420391500Z"
    }
   },
   "id": "53b572ec624b210"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 28,672 || all params: 6,738,444,288 || trainable%: 0.00042549880617192095\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model, hard_prompt_config)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T18:08:09.646025900Z",
     "start_time": "2023-09-30T18:08:09.466820500Z"
    }
   },
   "id": "499ce71895d4b9dc"
  },
  {
   "cell_type": "markdown",
   "id": "84871e0c-0768-4d23-8b5a-2b695f2a5c3e",
   "metadata": {},
   "source": [
    "## this is to make sure after truncation, all data end with eos\n",
    "## truncation is needed due to cuda oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51d2adda-ee97-4c2e-929d-7f1289e5d855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T01:52:39.856105600Z",
     "start_time": "2023-09-30T01:52:39.625725Z"
    }
   },
   "outputs": [],
   "source": [
    "def contain_eos_filter_llama2(data_point, dataset_text_field=\"text\"):\n",
    "    # 2 is eos for llama2\n",
    "    return tokenizer(data_point[dataset_text_field], padding=True, truncation=True, max_length=max_length)[\"input_ids\"][-1] == tokenizer.eos_token_id\n",
    "\n",
    "# Apply the filtering function\n",
    "train_dataset = train_dataset.filter(contain_eos_filter_llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28b2877-e80a-40c8-9c50-14c448845b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T01:52:39.875421900Z",
     "start_time": "2023-09-30T01:52:39.856105600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text'],\n    num_rows: 299\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f27ffe-2299-49df-aef7-07870d5ac9d5",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-30T02:05:32.159202Z",
     "start_time": "2023-09-30T01:52:39.875421900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/299 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a09f3bb66bd042fab3d8fe55c5447270"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/5000 : < :, Epoch 0.01/34]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = transformer_trainer(model, hard_prompt_config, tokenizer, train_dataset, dataset_text_field=\"text\", label_text_field=True, output_dir=\"llama2_results\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a9b86c-714d-4332-9829-62e960649670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4c59c-824d-476f-8f0c-35b18562fc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56fc3405-71e9-4f31-ad6f-0ce9e910966b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf40680-bd91-44e8-a069-7611c94c08ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(prompt,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        top_k=50, \n",
    "        eos_token_id=None):\n",
    "        inputs=tokenizer([prompt], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "\n",
    "        streamer = TextIteratorStreamer(tokenizer,\n",
    "                                        timeout=10.,\n",
    "                                        skip_prompt=True,\n",
    "                                        skip_special_tokens=True)\n",
    "        generate_kwargs = dict(\n",
    "            inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=96,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            temperature=temperature,\n",
    "            num_beams=1,\n",
    "            eos_token_id=eos_token_id,\n",
    "        )\n",
    "        t = Thread(target=trainer.model.generate, kwargs=generate_kwargs)\n",
    "        t.start()\n",
    "\n",
    "        outputs = []\n",
    "        for text in streamer:\n",
    "            outputs.append(text)\n",
    "            yield ''.join(outputs).lstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8e00e-f569-49b1-815a-0b0e39f6991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genearte_answer(prompt, eos_token_id=None):\n",
    "    generator = run(prompt, eos_token_id=eos_token_id)\n",
    "    previous_texts = \"\"\n",
    "    for response in generator:\n",
    "        print(response[len(previous_texts):], end='')\n",
    "        previous_texts = response\n",
    "    return previous_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fac077-4074-4a1e-9a9e-b03a9dd9a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_ids = []\n",
    "# for stuff, stuff_id in tokenizer.vocab.items():\n",
    "#     if stuff.endswith(\";\"):\n",
    "#         end_token_ids.append(stuff_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0432f23-cf97-4e79-bf88-3c3302dbf077",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 80\n",
    "print(test_dataset[\"text\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5123e0-f96e-4b67-ad06-82e92416d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset[\"output_text\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac1316b-d8aa-4b8d-bc84-de6d07a32f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = genearte_answer(test_dataset[\"text\"][index], eos_token_id=[tokenizer.eos_token_id]+end_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778062a-139c-4e6f-a939-6e9ad23d255b",
   "metadata": {},
   "source": [
    "# save adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280dd93-4bb0-4a3a-ab59-a3f8d097bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96816cb-1712-4040-b627-6b6e36657eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa282fe8-ac18-46bb-903b-79860646cb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f120b238-2df1-4e2d-8ac2-a26a36694b3e",
   "metadata": {},
   "source": [
    "# Merge weights and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df2487-8944-41f8-87ae-fa70eb5a0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-spider\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df8c5c-a4fa-4831-8fa8-d0dbf4937bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c7fb2-edd0-49d2-84c5-23626187bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('llama2-7b-merged-spider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d04c490-d2ea-4b8b-80f3-428b83f91165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad945216-955c-41d7-8973-fcbc0d4f20c1",
   "metadata": {},
   "source": [
    "# load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from utils import (\n",
    "    progress_generation, \n",
    "    load_merged,\n",
    "    load_base_and_adapter,\n",
    "    bnb_config\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6788a0386849bafb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad3ef23-1128-41d0-9ace-fc96c3c591ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a2ad8fe2745e2a276874dde8f01f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = load_merged(\"llama2-7b-merged-spider\", bnb_config)\n",
    "\n",
    "# model = load_base_and_adapter(\"meta-llama/Llama-2-7b-chat-hf\", \"llama-2-7b-spider\", quantization_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17a9de0-7501-44ec-ac58-75c77ea9918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98828e-7f1b-4a24-93b5-87f5a9aec10c",
   "metadata": {},
   "source": [
    "# make prediction using loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ca2e83-163f-496e-8d45-37dd4a27636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_token_ids = []\n",
    "for stuff, stuff_id in tokenizer.vocab.items():\n",
    "    if stuff.endswith(\";\"):\n",
    "        end_token_ids.append(stuff_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21e63fb-c38e-4094-a065-56d2aa9359c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Convert text to SQL:\n",
      "[Schema (values)]: continents : ContId, Continent | countries : CountryId, CountryName, Continent;\n",
      "[Column names (type)]: continents : ContId (number)| continents : Continent (text)| countries : CountryId (number)| countries : CountryName (text)| countries : Continent (number);\n",
      "[Q]: For each continent, list its id, name, and how many countries it has? [/INST]\n"
     ]
    }
   ],
   "source": [
    "index = 90\n",
    "print(test_dataset[\"text\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d805356-6846-4be4-b449-5c27f6e583e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT T1.contid , T1.continent , count(*) FROM continents AS T1 JOIN countries AS T2 ON T1.contid = T2.continent GROUP BY T1.contid;"
     ]
    }
   ],
   "source": [
    "response = progress_generation(test_dataset[\"text\"][index],\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    temperature=0.01,\n",
    "                    top_p=0.95,\n",
    "                    top_k=50,\n",
    "                    max_new_tokens=96,\n",
    "                    eos_token_id=[tokenizer.eos_token_id]+end_token_ids,\n",
    "                    show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de85b55-4684-4b6c-bbf6-2d8aebb22484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62827bea-6233-4118-8e14-9e6795d95223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb6ae4-57dd-432b-ae4e-2b78213e6599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
